name: Airflow ML Pipeline CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  AIRFLOW_UID: 50000
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__CORE__UNIT_TEST_MODE: 'true'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install apache-airflow==2.7.3 pandas scikit-learn numpy pytest

      - name: Initialize Airflow DB
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow_home
          export AIRFLOW__CORE__LOAD_EXAMPLES=false
          mkdir -p $AIRFLOW_HOME
          airflow db init

      - name: Lint DAG files
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow_home
          export AIRFLOW__CORE__LOAD_EXAMPLES=false
          python -c "
          from airflow.models import DagBag
          import sys
          
          dagbag = DagBag('dags', include_examples=False)
          
          if dagbag.import_errors:
              print('‚ùå DAG Import Errors:')
              for filename, error in dagbag.import_errors.items():
                  print(f'\nFile: {filename}')
                  print(f'Error: {error}')
              sys.exit(1)
          
          print('‚úÖ No import errors found')
          print(f'‚úÖ Total DAGs loaded: {len(dagbag.dags)}')
          "

      - name: Test DAG integrity
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow_home
          export AIRFLOW__CORE__LOAD_EXAMPLES=false
          python -c "
          from airflow.models import DagBag
          import sys
          
          dagbag = DagBag('dags', include_examples=False)
          
          if not dagbag.dags:
              print('‚ùå No DAGs found')
              sys.exit(1)
          
          errors = []
          for dag_id, dag in dagbag.dags.items():
              print(f'\nüìã Testing DAG: {dag_id}')
              
              if dag.task_count == 0:
                  errors.append(f'{dag_id} has no tasks')
                  continue
              
              print(f'  ‚úì Tasks: {dag.task_count}')
              for task in dag.tasks:
                  print(f'    - {task.task_id} ({task.task_type})')
              
              # Check for cycles
              try:
                  dag.test_cycle()
                  print(f'  ‚úì No cycles detected')
              except Exception as e:
                  errors.append(f'{dag_id} has cycles: {e}')
          
          if errors:
              print('\n‚ùå Validation Errors:')
              for error in errors:
                  print(f'  - {error}')
              sys.exit(1)
          
          print('\n‚úÖ All DAGs validated successfully!')
          "

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create .env file
        run: |
          echo "AIRFLOW_UID=${{ env.AIRFLOW_UID }}" > .env
          echo "_AIRFLOW_WWW_USER_USERNAME=airflow" >> .env
          echo "_AIRFLOW_WWW_USER_PASSWORD=airflow" >> .env

      - name: Create required directories
        run: |
          mkdir -p logs dags plugins data

      - name: Start Docker Compose services
        run: |
          docker-compose up -d
          echo "Waiting for services to be healthy..."
          sleep 60

      - name: Check services health
        run: |
          docker-compose ps
          echo "=== Airflow Webserver Logs ==="
          docker-compose logs airflow-webserver | tail -30
          echo "=== Airflow Scheduler Logs ==="
          docker-compose logs airflow-scheduler | tail -30

      - name: Verify Airflow is running
        run: |
          max_attempts=10
          attempt=0
          until curl -f http://localhost:8080/health || [ $attempt -eq $max_attempts ]; do
            attempt=$((attempt + 1))
            echo "Attempt $attempt/$max_attempts - Waiting for Airflow..."
            sleep 10
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo "‚ùå Airflow failed to start"
            docker-compose logs
            exit 1
          fi
          
          echo "‚úÖ Airflow is healthy"

      - name: List DAGs in Airflow
        run: |
          docker-compose exec -T airflow-scheduler airflow dags list || true

      - name: Stop services
        if: always()
        run: docker-compose down -v

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy notification
        run: |
          echo "üöÄ Deployment to production would happen here"
          echo "üì¶ Artifacts ready for deployment"
          echo "‚úÖ CI/CD pipeline completed successfully"

      # Uncomment and configure for actual deployment
      # - name: Deploy to production server
      #   uses: appleboy/ssh-action@v1.0.0
      #   with:
      #     host: ${{ secrets.PROD_HOST }}
      #     username: ${{ secrets.PROD_USER }}
      #     key: ${{ secrets.SSH_PRIVATE_KEY }}
      #     script: |
      #       cd /path/to/airflow-iris-etl
      #       git pull origin main
      #       docker-compose down
      #       docker-compose up -d